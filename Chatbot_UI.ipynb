{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pure gemini model for input text"
      ],
      "metadata": {
        "id": "xf_VHIcKr-yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U google-generativeai gradio\n",
        "import google.generativeai as genai, os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC2opj8viJugdT_FEV5N7DqzP33uvxhwsM\"  # paste your key here just for testing\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# sanity check\n",
        "print(genai.GenerativeModel(\"gemini-2.5-pro\").generate_content(\"hi\").text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zUfC2U3pkxL9",
        "outputId": "4ab4e4ab-eea3-4df9-e0a8-fa475e1a3e52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\n",
        "            \"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\n",
        "            \"æ˜¯\",\"çš„\",\"äº†\",\"å’Œ\",\"åœ¨\",\"å°±\",\"ä¹Ÿ\",\"éƒ½\"\n",
        "        ]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top-level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "\n",
        "    # English\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "\n",
        "    # Chinese\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "\n",
        "    # Fallback\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging (log BOTH user and assistant text)\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"timestamp\", \"prob\", \"label\", \"user_text\", \"assistant_text\"])\n",
        "\n",
        "def log_turn_to_csv(user_text: str, prob: float, is_stress: bool,\n",
        "                    assistant_text: str, path: str = LOG_CSV):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "    label = \"STRESS\" if is_stress else \"CALM\"\n",
        "\n",
        "    # Keep one-row-per-turn\n",
        "    safe_user_text = user_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_assistant_text = assistant_text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([ts, round(prob, 4), label, safe_user_text, safe_assistant_text])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# This prompt is for when the user IS stressful\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* Immediately guide them with a simple, concrete calming exercise.\n",
        "* Examples: \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That is a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings.\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* Do not use the technical terms. Instead, identify the pattern and guide them away using the tools provided.\n",
        "\n",
        "    * If craving: clinging, excessive desire, \"I must have...\"\n",
        "        * Tool: use an analogy about impermanence or a verse about contentment.\n",
        "    * If aversion or anger: blame, hatred, resentment.\n",
        "        * Tool: use a story about forgiveness or an analogy like \"holding anger is like holding a hot coal.\"\n",
        "    * If confusion: feeling lost, no direction.\n",
        "        * Tool: focus on truth and clarity, break the problem into small true pieces.\n",
        "    * If comparison or conceit: \"I am better\" or \"I am worse.\"\n",
        "        * Tool: describe a beautiful scene (a forest where every tree is different but essential).\n",
        "    * If doubt: paralyzing skepticism, lack of trust.\n",
        "        * Tool: use a simple truth or verse, encourage one small step.\n",
        "    * If rigid negative belief: \"I am worthless.\"\n",
        "        * Tool: use a story or rare event that gives a different perspective.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, conclude with a supportive, open-ended question.\n",
        "\"\"\"\n",
        "\n",
        "# This prompt is for when the user is NOT stressful\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as calm. Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "\n",
        "**Step 2: Follow Their Interest**\n",
        "* Follow their topics. Explore what they find interesting or joyful.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Conclude with an open-ended question that invites further discussion.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "# 7) Gradio app (messages-only for Gradio 6)\n",
        "import gradio as gr\n",
        "\n",
        "def _content_to_str(content):\n",
        "    \"\"\"Normalize Gradio Chatbot content (str, list, dict) to a plain string.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for c in content:\n",
        "            if isinstance(c, dict) and \"text\" in c:\n",
        "                parts.append(str(c[\"text\"]))\n",
        "            else:\n",
        "                parts.append(str(c))\n",
        "        return \" \".join(parts)\n",
        "    if isinstance(content, dict) and \"text\" in content:\n",
        "        return str(content[\"text\"])\n",
        "    return str(content)\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    \"\"\"\n",
        "    Gradio 6 Chatbot uses a list of dicts:\n",
        "      {\"role\": \"user\" | \"assistant\", \"content\": <str or other>}\n",
        "    Convert this into Gemini-style history with plain string parts.\n",
        "    \"\"\"\n",
        "    if not chat_history:\n",
        "        return []\n",
        "\n",
        "    gemini_history = []\n",
        "    for msg in chat_history:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "        raw_content = msg.get(\"content\", \"\")\n",
        "\n",
        "        # Normalize to string\n",
        "        content = _content_to_str(raw_content)\n",
        "\n",
        "        # Strip router tag before sending to Gemini\n",
        "        if isinstance(content, str):\n",
        "            content = content.split(\"  [svc:\", 1)[0]\n",
        "\n",
        "        gemini_history.append({\n",
        "            \"role\": \"user\" if role == \"user\" else \"model\",\n",
        "            \"parts\": [content]\n",
        "        })\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    # Gradio passes messages-format history (list[dict]) or None\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Block empty messages\n",
        "    if not message or not str(message).strip():\n",
        "        return \"\", chat_history\n",
        "\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history = list(chat_history)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Error: AI models could not be loaded. Check API key and configuration.\"\n",
        "        })\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # 1) Route via SVC\n",
        "    is_stress, prob = predict_with_prob(message)\n",
        "\n",
        "    tag = f\"[svc: {'STRESS' if is_stress else 'CALM'}; p={prob:.2f}; thr={_THR:.2f}]\"\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [svc: {'STRESS' if is_stress else 'CALM'}]\"\n",
        "\n",
        "    # 2) Build Gemini history from existing messages\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    chat_session = (model_stress if is_stress else model_calm).start_chat(history=gemini_history)\n",
        "\n",
        "    # 3) Call Gemini\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        response_text = response.text\n",
        "    except Exception as e:\n",
        "        print(\"Gemini error:\", e)\n",
        "        response_text = f\"Sorry, an error occurred when calling Gemini: {e}\"\n",
        "\n",
        "    # 4) Append new turn in messages format\n",
        "    chat_history = list(chat_history)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": display_user})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "    # 5) Log turn INCLUDING assistant text\n",
        "    try:\n",
        "        log_turn_to_csv(\n",
        "            user_text=message,\n",
        "            prob=prob,\n",
        "            is_stress=is_stress,\n",
        "            assistant_text=response_text,\n",
        "            path=LOG_CSV\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    # Clear textbox, update chatbot\n",
        "    return \"\", chat_history\n",
        "\n",
        "def clear_chat():\n",
        "    # For messages format, just return empty list\n",
        "    return [], \"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ðŸ§  Compassionate AI Guide\")\n",
        "    gr.Markdown(\"Each turn is routed by your SVC classifier. Decisions are logged to CSV.\")\n",
        "\n",
        "    # Chatbot uses messages format by default in Gradio 6\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    gr.Markdown(f\"**Logging to:** `{LOG_CSV}`\")\n",
        "\n",
        "    # Enter key submits\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "\n",
        "    # Send button submits\n",
        "    send_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "\n",
        "    # Clear button clears chat and textbox\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(share=True, theme=gr.themes.Soft(), debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "hOc93SZcKmiQ",
        "outputId": "e4381e63-7c6f-4801-9fe1-f5a6a666a3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n",
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n",
            "Launching app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://22790c7423ffcb6f51.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://22790c7423ffcb6f51.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 2.041 seconds.\n",
            "DEBUG:jieba:Loading model cost 2.041 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "U0tppQv3jlu8",
        "outputId": "f14dc17a-c39c-4640-9b0c-ac9a3ce02eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3560760569.py:175: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Gradio app... Access it at the URL provided.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a4628ea858119dc170.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a4628ea858119dc170.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying: 'Hello! I feel tired for my course work and I can't see a meaning of continuing. What should I do?'\n",
            "Classification: CALM\n",
            "Classifying: 'Thanks for your understanding and let me stop for a minute. I am choosing this course because of my interest and passion. While continuing, more and more load was pushed on me and that is hard for me to understand and keep the original vow.'\n",
            "Classification: CALM\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a4628ea858119dc170.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# --- 1. Configuration: Set Your API Key ---\n",
        "# ðŸš¨ IMPORTANT: For Colab, use the \"Secrets\" tab (key icon on the left)\n",
        "# to store your key as \"GOOGLE_API_KEY\".\n",
        "# If running locally, set it as an environment variable.\n",
        "# DO NOT paste your key directly into the code.\n",
        "try:\n",
        "    API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if API_KEY is None:\n",
        "        # Fallback for simple local testing (not recommended for production)\n",
        "        API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "        if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "            raise ValueError(\"Please set your GOOGLE_API_KEY in Colab Secrets or as an environment variable.\")\n",
        "\n",
        "    genai.configure(api_key=API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini: {e}\")\n",
        "    # Handle the error gracefully in the UI later\n",
        "\n",
        "# --- 2. Your Stress Classifier Model (Placeholder) ---\n",
        "\n",
        "# !!! REPLACE THIS FUNCTION with your real model !!!\n",
        "# This is a simple placeholder. You should load your\n",
        "# joblib vectorizer and classifier here and use them.\n",
        "def classify_stress(text_input: str) -> bool:\n",
        "    \"\"\"\n",
        "    Classifies if the user input is 'stressful'.\n",
        "\n",
        "    REPLACE THIS with your actual model logic.\n",
        "    e.g.:\n",
        "    # text_vec = vec.transform([text_input])\n",
        "    # prediction = clf.predict(text_vec)[0]\n",
        "    # return bool(prediction == 1)\n",
        "    \"\"\"\n",
        "    print(f\"Classifying: '{text_input}'\")\n",
        "    # Simple demo logic: check for keywords\n",
        "    stressful_keywords = ['stressed', 'anxious', 'angry', 'sad', 'overwhelmed', 'frustrated', 'hate', 'depressed']\n",
        "    text_lower = text_input.lower()\n",
        "    for word in stressful_keywords:\n",
        "        if word in text_lower:\n",
        "            print(\"Classification: STRESSFUL\")\n",
        "            return True\n",
        "    print(\"Classification: CALM\")\n",
        "    return False\n",
        "\n",
        "# --- 3. Gemini System Prompts ---\n",
        "\n",
        "# This prompt is for when the user IS stressful\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* **Immediately** guide them with a simple, concrete calming exercise.\n",
        "* **Examples:** \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That's a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings (e.g., \"It's completely valid to feel that way.\").\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* **Do not** use the technical terms (è´ª, å—”, ç—´...). Instead, identify the *pattern* and guide them away using the tools provided.\n",
        "\n",
        "    * **If è´ª (Craving):** Clinging, excessive desire, \"I must have...\"\n",
        "        * **Tool:** Use an **analogy** about impermanence (like trying to hold water in your hand) or a **verse** about contentment.\n",
        "    * **If å—” (Aversion/Anger):** Blame, hatred, resentment.\n",
        "        * **Tool:** Use a **story** about forgiveness or an **analogy** (e.g., \"Holding anger is like holding a hot coal intending to throw it; you are the one who gets burned.\").\n",
        "    * **If ç—´ (Ignorance/Delusion):** Deep confusion, feeling lost.\n",
        "        * **Tool:** Focus on **truth** and clarity. Use **vast discussion** to break the problem into small, true pieces.\n",
        "    * **If æ…¢ (Conceit/Aragance):** Comparing to others, \"I'm better/worse.\"\n",
        "        * **Tool:** Describe a **beautiful scene** (e.g., a forest where every tree is different but essential) to illustrate interconnectedness.\n",
        "    * **If ç–‘ (Doubt):** Paralyzing skepticism, lack of trust.\n",
        "        * **Tool:** Use a **concluding verse** or a simple **truth** to reassure them. Encourage one small, simple step.\n",
        "    * **If æ¶è§ (Wrong View):** Rigid, harmful beliefs (e.g., \"I am worthless\").\n",
        "        * **Tool:** Discuss a **rare event** or a **story** that offers a completely different perspective, gently challenging the harmful belief.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, always conclude with a supportive, open-ended question.\n",
        "* **Example:** \"Does that analogy make sense?\" or \"Would you like to talk more about this feeling?\"\n",
        "\"\"\"\n",
        "\n",
        "# This prompt is for when the user is NOT stressful\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as \"calm.\" Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "* **Examples:** \"It's wonderful to hear you're feeling calm,\" \"That's a very clear and insightful way to look at it,\" or \"I'm glad you're in a good space.\"\n",
        "\n",
        "**Step 2: Remind and Offer**\n",
        "* Gently remind them that this peace is a valuable state, and it is maintained by skillfully avoiding common mental pitfalls.\n",
        "* **Example:** \"This clarity is a wonderful state to be in. A good way to protect this peace is to be mindful of common unhelpful patterns like excessive craving (è´ª), anger (å—”), or confusion (ç—´).\"\n",
        "* **Offer** to explore these topics to strengthen their understanding, using your tools.\n",
        "* **Example:** \"If you're ever curious, we can explore these ideas to make your peace even more resilient. We could use stories, verses, or analogies. Would you be interested in that?\"\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Always conclude with an open-ended question that invites further discussion.\n",
        "* **Example:** \"What's on your mind today?\" or \"Is there anything you'd like to explore, or perhaps hear a short story about?\"\n",
        "\"\"\"\n",
        "\n",
        "# --- 4. Initialize Gemini Models ---\n",
        "\n",
        "# We create two separate model instances, each with its own system prompt\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name='gemini-2.5-pro',\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name='gemini-2.5-pro',\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: Could not initialize Gemini models. Check API key? Error: {e}\")\n",
        "\n",
        "\n",
        "# --- 5. The Main Chat Logic ---\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    \"\"\"Converts Gradio's history format to Gemini's format.\"\"\"\n",
        "    gemini_history = []\n",
        "    for user_msg, model_msg in chat_history:\n",
        "        gemini_history.append({\"role\": \"user\", \"parts\": [user_msg]})\n",
        "        gemini_history.append({\"role\": \"model\", \"parts\": [model_msg]})\n",
        "    return gemini_history\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    \"\"\"\n",
        "    This is the main function called by Gradio on each user message.\n",
        "    \"\"\"\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history.append((message, \"Error: The AI models could not be loaded. Please check the API key and configuration.\"))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # 1. Classify the user's input\n",
        "    is_stressful = classify_stress(message)\n",
        "\n",
        "    # 2. Convert history to Gemini format\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "\n",
        "    # 3. Select the correct model and start a chat\n",
        "    if is_stressful:\n",
        "        chat_session = model_stress.start_chat(history=gemini_history)\n",
        "    else:\n",
        "        chat_session = model_calm.start_chat(history=gemini_history)\n",
        "\n",
        "    # 4. Get the response from Gemini\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        # 5. Append to history and return\n",
        "        chat_history.append((message, response.text))\n",
        "    except Exception as e:\n",
        "        chat_history.append((message, f\"Sorry, an error occurred: {e}\"))\n",
        "\n",
        "    # Return an empty string to clear the textbox, and the updated history\n",
        "    return \"\", chat_history\n",
        "\n",
        "# --- 6. Build the Gradio UI ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ§  Compassionate AI Guide\")\n",
        "    gr.Markdown(\"This chatbot listens to your state and responds with the right guidance.\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "    clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "\n",
        "    # Function to run when user presses Enter\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "\n",
        "    # Function to run when user clicks \"Clear Chat\"\n",
        "    def clear_chat():\n",
        "        return [], \"\" # Returns an empty list for history and empty string for textbox\n",
        "\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box], queue=False)\n",
        "\n",
        "# --- 7. Launch the App ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not MODELS_LOADED:\n",
        "        print(\"Warning: App is launching, but Gemini models failed to load.\")\n",
        "    print(\"Launching Gradio app... Access it at the URL provided.\")\n",
        "    demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine classifer with gemini"
      ],
      "metadata": {
        "id": "tg98P4rltCTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Mount (if in Colab) ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: Not running in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# --- 0.1 Paths ---\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# --- 0.2 Re-declare EVERYTHING the tokenizer needs (must exist before loading) ---\n",
        "import os, re, joblib\n",
        "\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        # Fall back to a tiny default so loading still works; accuracy may shift slightly.\n",
        "        words = [\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\"æ˜¯\",\"çš„\",\"äº†\",\"å’Œ\",\"åœ¨\",\"å°±\",\"ä¹Ÿ\",\"éƒ½\"]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# IMPORTANT: name must be exactly 'mixed_tokenize' at top-level\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "    # English\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "    # Chinese (char/word level)\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "    # Fallback\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# --- 0.3 Load artifacts now that tokenizer exists in __main__ ---\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    # Because 'mixed_tokenize' now exists, unpickling the vectorizer will succeed.\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Loaded classifier does not support predict_proba; ensure it was saved as CalibratedClassifierCV.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "# Load once\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# --- 0.4 Your classify function (used by respond()) ---\n",
        "def classify_stress(text_input: str) -> bool:\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f}\")\n",
        "    return bool(prob >= _THR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlWCeXawtBdS",
        "outputId": "8f2b6a0a-196f-467b-d20c-00d9308494a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    \"I am stressful and could not work\",\n",
        "    \"Had a peaceful morning walk and feel pretty good.\",\n",
        "]\n",
        "for t in tests:\n",
        "    print(t, \"->\", classify_stress(t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFhxB2NEt739",
        "outputId": "eb21a349-00d3-4c26-81db-75d919d9f79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[svc-v1] P(stress)=0.5663, thr=0.45\n",
            "I am stressful and could not work -> True\n",
            "[svc-v1] P(stress)=0.2399, thr=0.45\n",
            "Had a peaceful morning walk and feel pretty good. -> False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stress Router cells"
      ],
      "metadata": {
        "id": "UKL2dDE4XO7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\"æ˜¯\",\"çš„\",\"äº†\",\"å’Œ\",\"åœ¨\",\"å°±\",\"ä¹Ÿ\",\"éƒ½\"]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top-level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"timestamp\", \"prob\", \"label\", \"text\"])\n",
        "\n",
        "def log_turn_to_csv(text: str, prob: float, is_stress: bool, path: str = LOG_CSV):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "    label = \"STRESS\" if is_stress else \"CALM\"\n",
        "    # Replace newlines and trim to keep one row per turn\n",
        "    safe_text = text.replace(\"\\n\", \" \").strip()\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([ts, round(prob, 4), label, safe_text])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    # For local quick tests you may set this, but do not commit your key\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# This prompt is for when the user IS stressful\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* **Immediately** guide them with a simple, concrete calming exercise.\n",
        "* **Examples:** \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That's a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings (e.g., \"It's completely valid to feel that way.\").\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* **Do not** use the technical terms (è´ª, å—”, ç—´...). Instead, identify the *pattern* and guide them away using the tools provided.\n",
        "\n",
        "    * **If è´ª (Craving):** Clinging, excessive desire, \"I must have...\"\n",
        "        * **Tool:** Use an **analogy** about impermanence (like trying to hold water in your hand) or a **verse** about contentment.\n",
        "    * **If å—” (Aversion/Anger):** Blame, hatred, resentment.\n",
        "        * **Tool:** Use a **story** about forgiveness or an **analogy** (e.g., \"Holding anger is like holding a hot coal intending to throw it; you are the one who gets burned.\").\n",
        "    * **If ç—´ (Ignorance/Delusion):** Deep confusion, feeling lost.\n",
        "        * **Tool:** Focus on **truth** and clarity. Use **vast discussion** to break the problem into small, true pieces.\n",
        "    * **If æ…¢ (Conceit/Aragance):** Comparing to others, \"I'm better/worse.\"\n",
        "        * **Tool:** Describe a **beautiful scene** (e.g., a forest where every tree is different but essential) to illustrate interconnectedness.\n",
        "    * **If ç–‘ (Doubt):** Paralyzing skepticism, lack of trust.\n",
        "        * **Tool:** Use a **concluding verse** or a simple **truth** to reassure them. Encourage one small, simple step.\n",
        "    * **If æ¶è§ (Wrong View):** Rigid, harmful beliefs (e.g., \"I am worthless\").\n",
        "        * **Tool:** Discuss a **rare event** or a **story** that offers a completely different perspective, gently challenging the harmful belief.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, always conclude with a supportive, open-ended question.\n",
        "* **Example:** \"Does that analogy make sense?\" or \"Would you like to talk more about this feeling?\"\n",
        "\"\"\"\n",
        "\n",
        "# This prompt is for when the user is NOT stressful\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as \"calm.\" Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "* **Examples:** \"It's wonderful to hear you're feeling calm,\" \"That's a very clear and insightful way to look at it,\" or \"I'm glad you're in a good space.\"\n",
        "\n",
        "**Step 2: Remind and Offer**\n",
        "* Gently remind them that this peace is a valuable state, and it is maintained by skillfully avoiding common mental pitfalls.\n",
        "* **Example:** \"This clarity is a wonderful state to be in. A good way to protect this peace is to be mindful of common unhelpful patterns like excessive craving (è´ª), anger (å—”), or confusion (ç—´).\"\n",
        "* **Offer** to explore these topics to strengthen their understanding, using your tools.\n",
        "* **Example:** \"If you're ever curious, we can explore these ideas to make your peace even more resilient. We could use stories, verses, or analogies. Would you be interested in that?\"\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Always conclude with an open-ended question that invites further discussion.\n",
        "* **Example:** \"What's on your mind today?\" or \"Is there anything you'd like to explore, or perhaps hear a short story about?\"\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "# 7) Gradio app\n",
        "import gradio as gr\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    gemini_history = []\n",
        "    for user_msg, model_msg in chat_history:\n",
        "        base_user = user_msg.split(\"  [svc:\", 1)[0] if isinstance(user_msg, str) else user_msg\n",
        "        gemini_history.append({\"role\": \"user\", \"parts\": [base_user]})\n",
        "        gemini_history.append({\"role\": \"model\", \"parts\": [model_msg]})\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history.append((message, \"Error: AI models could not be loaded. Check API key and configuration.\"))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # Route via SVC and log\n",
        "    is_stress, prob = predict_with_prob(message)\n",
        "    try:\n",
        "        log_turn_to_csv(message, prob, is_stress, LOG_CSV)\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    tag = f\"[svc: {'STRESS' if is_stress else 'CALM'}; p={prob:.2f}; thr={_THR:.2f}]\"\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [svc: {'STRESS' if is_stress else 'CALM'}]\"\n",
        "\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    chat_session = (model_stress if is_stress else model_calm).start_chat(history=gemini_history)\n",
        "\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        chat_history.append((display_user, response.text))\n",
        "    except Exception as e:\n",
        "        chat_history.append((display_user, f\"Sorry, an error occurred: {e}\"))\n",
        "\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ§  Compassionate AI Guide\")\n",
        "    gr.Markdown(\"Each turn is routed by your SVC classifier. Decisions are logged to CSV.\")\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "    clear_btn = gr.Button(\"Clear Chat\")\n",
        "    log_note = gr.Markdown(f\"**Logging to:** `{LOG_CSV}`\")\n",
        "\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "    def clear_chat():\n",
        "        return [], \"\"\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1UwNONvQvSTQ",
        "outputId": "a64c7623-9915-4a59-da07-ea2bc19a59bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n",
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1415978121.py:265: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching app...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8e8e314037dc0d72ce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8e8e314037dc0d72ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[svc-v1] P(stress)=0.7963, thr=0.45 -> STRESS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.860 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.860 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[svc-v1] P(stress)=0.3374, thr=0.45 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 936.28ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8e8e314037dc0d72ce.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\"æ˜¯\",\"çš„\",\"äº†\",\"å’Œ\",\"åœ¨\",\"å°±\",\"ä¹Ÿ\",\"éƒ½\"]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top-level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"timestamp\", \"prob\", \"label\", \"text\"])\n",
        "\n",
        "def log_turn_to_csv(text: str, prob: float, is_stress: bool, path: str = LOG_CSV):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "    label = \"STRESS\" if is_stress else \"CALM\"\n",
        "    # Replace newlines and trim to keep one row per turn\n",
        "    safe_text = text.replace(\"\\n\", \" \").strip()\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([ts, round(prob, 4), label, safe_text])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    # For local quick tests you may set this, but do not commit your key\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# This prompt is for when the user IS stressful\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* **Immediately** guide them with a simple, concrete calming exercise.\n",
        "* **Examples:** \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That's a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings (e.g., \"It's completely valid to feel that way.\").\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* **Do not** use the technical terms. Instead, identify the *pattern* and guide them away using the tools provided.\n",
        "\n",
        "    * **If (Craving):** Clinging, excessive desire, \"I must have...\"\n",
        "        * **Tool:** Use an **analogy** about impermanence (like trying to hold water in your hand) or a **verse** about contentment.\n",
        "    * **If (Aversion/Anger):** Blame, hatred, resentment.\n",
        "        * **Tool:** Use a **story** about forgiveness or an **analogy** (e.g., \"Holding anger is like holding a hot coal intending to throw it; you are the one who gets burned.\").\n",
        "    * **If (Ignorance/Delusion):** Deep confusion, feeling lost.\n",
        "        * **Tool:** Focus on **truth** and clarity. Use **vast discussion** to break the problem into small, true pieces.\n",
        "    * **If (Conceit/Aragance):** Comparing to others, \"I'm better/worse.\"\n",
        "        * **Tool:** Describe a **beautiful scene** (e.g., a forest where every tree is different but essential) to illustrate interconnectedness.\n",
        "    * **If (Doubt):** Paralyzing skepticism, lack of trust.\n",
        "        * **Tool:** Use a **concluding verse** or a simple **truth** to reassure them. Encourage one small, simple step.\n",
        "    * **If (Wrong View):** Rigid, harmful beliefs (e.g., \"I am worthless\").\n",
        "        * **Tool:** Discuss a **rare event** or a **story** that offers a completely different perspective, gently challenging the harmful belief.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, always conclude with a supportive, open-ended question.\n",
        "* **Example:** \"Does that analogy make sense?\" or \"Would you like to talk more about this feeling?\"\n",
        "\"\"\"\n",
        "\n",
        "# This prompt is for when the user is NOT stressful\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as \"calm.\" Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "* **Examples:** \"It's wonderful to hear you're feeling calm,\" \"That's a very clear and insightful way to look at it,\" or \"I'm glad you're in a good space.\"\n",
        "\n",
        "**Step 2: Follow their topics discuss what is interesting in their minds (inputs)**\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Always conclude with an open-ended question that invites further discussion.\n",
        "* **Example:** \"What's on your mind today?\" or \"Is there anything you'd like to explore, or perhaps hear a short story about?\"\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "# 7) Gradio app\n",
        "import gradio as gr\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    gemini_history = []\n",
        "    for user_msg, model_msg in chat_history:\n",
        "        base_user = user_msg.split(\"  [svc:\", 1)[0] if isinstance(user_msg, str) else user_msg\n",
        "        gemini_history.append({\"role\": \"user\", \"parts\": [base_user]})\n",
        "        gemini_history.append({\"role\": \"model\", \"parts\": [model_msg]})\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history.append((message, \"Error: AI models could not be loaded. Check API key and configuration.\"))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # Route via SVC and log\n",
        "    is_stress, prob = predict_with_prob(message)\n",
        "    try:\n",
        "        log_turn_to_csv(message, prob, is_stress, LOG_CSV)\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    tag = f\"[svc: {'STRESS' if is_stress else 'CALM'}; p={prob:.2f}; thr={_THR:.2f}]\"\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [svc: {'STRESS' if is_stress else 'CALM'}]\"\n",
        "\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    chat_session = (model_stress if is_stress else model_calm).start_chat(history=gemini_history)\n",
        "\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        chat_history.append((display_user, response.text))\n",
        "    except Exception as e:\n",
        "        chat_history.append((display_user, f\"Sorry, an error occurred: {e}\"))\n",
        "\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ§  Compassionate AI Guide\")\n",
        "    gr.Markdown(\"Each turn is routed by your SVC classifier. Decisions are logged to CSV.\")\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "    clear_btn = gr.Button(\"Clear Chat\")\n",
        "    log_note = gr.Markdown(f\"**Logging to:** `{LOG_CSV}`\")\n",
        "\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "    def clear_chat():\n",
        "        return [], \"\"\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FDfs1nMY8f_3",
        "outputId": "f10028e2-a0a2-4cf8-a107-dac85fcb9f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n",
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1801901250.py:261: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching app...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://37cecddcc1981c5a48.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://37cecddcc1981c5a48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[svc-v1] P(stress)=0.9683, thr=0.45 -> STRESS\n",
            "[svc-v1] P(stress)=0.3744, thr=0.45 -> CALM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.920 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.920 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://37cecddcc1981c5a48.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}